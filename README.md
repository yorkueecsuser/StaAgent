# LLM for Testing Static Analyzers
This is the repo for the paper entitled **StaAgent: An Agentic Framework for Testing Static Analyzers** submitted to ICSE 2026.

In this work, we study five static analyzers for Java, [SpotBugs](https://spotbugs.github.io/), [SonarQube](https://www.sonarsource.com/products/sonarqube/), [ErrorProne](https://errorprone.info/), [Infer](https://fbinfer.com/), and [PMD](https://docs.pmd-code.org/latest/index.html).
The rules we select for our study are based on correctness, security vulnerabilities, and performance. These rules can be found in
`Rules_List/` directory in `spotbugs_rules.json`, `sonar_rules.json`, `prone_rules.json`, `infer_rules.json`, and `pmd_rules.json` files.
Each rule is given an id (starting from 1) that we use throughout the repo to refer to a particular rule from a tool.

StaAgent tests the reliability and robustness of the tools by employing an end-to-end automated process to find potential bugs in the rule implementations. StaAgent found 64 unique rule violations across the 5 static analyzer studied in this work. The bugs are reported to respective developers and the issue links, along with their fixing updates, are given in [StaAgent_Issue_List.md](https://github.com/ennorom/LLM-For-Testing-Static-Tools/blob/main/StaAgent_Issue_List.md) file.

## Run StaAgent
To run our pipeline, first install the following packages.
```
torch==2.5.1
transformers==4.47.0
accelerate==1.3.0
```
Then, run the `main.py` file with `--tool` and `--model` option to run it for a particular model and tool. For example, to run `SpotBugs` on `Qwen2.5-Coder-32B` model, run the following command.
```bash
python main.py --tool spotbugs --model qwen
```

After this finishes running, you have all the seeds and mutants generated. Now run the following command to run SpotBugs on these tool for metamorphic testing.
```bash
cd Running_Tools && \
    python run-spotbugs.py --model qwen
```

Finally, to check which seeds and mutants produced inconsistent results, run the following command-
```bash
cd DataAnalysis && \
    python count_tool_bugs.py --tool spotbugs --model qwen
```
To run other models or tools, you can use the following ids:
```txt
models: ["codellama", "codestral", "deepseek", "gpt-4o", "qwen"]
```
```txt
tools: ["spotbugs", "sonar", "prone", "infer", "pmd"]
```
If you want to add any other HuggingFace model, then just add that to the `agents/llm_utils.py` file.

## Code Generation
### Generated Files
The full filepath to a generated file from root is- 
```
GeneratedCode/{model-name}/{tool}_generated_code/result_{tool}_{rule_id}/{filename}
```
For example, for a code generated by Codestral, for ErrorProne rule id 102, the seed file is- 
```
GeneratedCode/Codestral/prone_generated_code/result_Prone_102/seed_Prone_102.java
```
The filename for the test file is `test_Prone_102.java`. A metadata file is also stored alongside the seed file and test file
that stores related information about file compilation status, test status, validation status, etc. and can be found
in the same directory with the name `data_Prone_102.json`.

The mutants we generate from the seed files can also be found
inside the same directory. We have 9 different mutant operators (OP1 to OP9), and for each operator, we generate 3 mutants.
The mutant files are named in the format `mutant_[1-3]_OP[1-9].java`. So, the second mutant for operator 4 will be
named `mutant_2_OP4.java`.
### Prompts
The prompts we use to generate the seed, test, and the mutants, and also the prompts to validate them can be found
inside `Prompts/` directory.

[//]: # (## Replicating Experiments)

[//]: # (### Environment setup)

[//]: # (Create a virtual environment with your choice of tool. We suggest using conda.)

[//]: # ()
[//]: # (Then install the following packages to be able to run the LLMs.)

[//]: # (```)

[//]: # (torch==2.5.1)

[//]: # (transformers==4.47.0)

[//]: # (accelerate==1.3.0   # for automatic GPU distribution if using multi-GPU)

[//]: # (```)

[//]: # (## Bug List)
[//]: # (The specific rules where we found the bugs are given inside `BugList/` directory. They are further separated in)
[//]: # (Type1 and Type2 folders. For the buggy rule with `id=5` from SpotBugs, the rule name, link to rule page,)
[//]: # (scan-results logs violating the rules are given in the following file- `BugList/Type1/SpotBugs/5/rule_bug_details.json`.)
[//]: # (The json file has the following structure-)
[//]: # (```)
[//]: # ({)
[//]: # (    "rule_id": <RULE-ID>,)
[//]: # (    "rule_name": <RULE-NAME>,)
[//]: # (    "rule_link": <LINK-TO-OFFICIAL-RULE-PAGE>,)
[//]: # (    "seed_file_path": "path/from/root/to/seed.java",)
[//]: # (    "seed_scan_logs": <SEED-SCAN-LOGS>,)
[//]: # (    "violating_mutants": [)
[//]: # (        {)
[//]: # (            "file_path": "path/from/root/to/mutant_x_OPy.java",)
[//]: # (            "scan_logs": <MUTANT-SCAN-LOGS>)
[//]: # (        })
[//]: # (    ])
[//]: # (})
[//]: # (```)

[//]: # (## Running Scans on Buggy Rules
All our analysis results can be accessed in `AnalysisResult/` directory. It uses the same `rule_id` in the filename as above.)

[//]: # (Also, the seed/mutant files we add in the `BugList/` directory can be used to verify our scan results for the buggy rules. Spotbugs do not generate any output if no bug is detected, so some of the `scan_logs` maybe empty, while some may show other bugs than the specific rule under study.)
<!-- ### ![](https://avatars.githubusercontent.com/u/23269302?s=16&v=4) Running SpotBugs
We used the latest SpotBugs version available during our evaluation- `4.9.2`. The it can be downloaded from this [tar link](https://github.com/spotbugs/spotbugs/releases/download/4.9.2/spotbugs-4.9.2.tgz).
or [zip link](https://github.com/spotbugs/spotbugs/releases/download/4.9.2/spotbugs-4.9.2.zip) for Windows.
After downloading the appropriate binary for your system, extract it with your favorite zip extractor.

After extracting it, add the Spotbugs JAR to environment variable. For linux you can do the following- 
```
export SPOTBUGS_JAR=/path/to/spotbugs-location/spotbugs-4.9.2/lib/spotbugs.jar
```

Then run the following command to scan a particular java file named `seed_Prone_102.java`-
```
javac /path/to/java-file/seed_Prone_102.java && java -jar $SPOTBUGS_JAR -low -effort:max /path/to/java-file/*.class
```
You should be able to see the scan results in the console after the command finishes.

### ðŸž Running ErrorProne
Download the [ErrorProne core](https://repo1.maven.org/maven2/com/google/errorprone/error_prone_core/2.36.0/error_prone_core-2.36.0-with-dependencies.jar) and [Dataflow-ErrorProne](https://repo1.maven.org/maven2/io/github/eisop/dataflow-errorprone/3.42.0-eisop5/dataflow-errorprone-3.42.0-eisop5.jar) JARs. Then run the following command to scan a java file
named `seed_Prone_102.java`-
```
javac \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.main=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.model=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.processing=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED \
    -J--add-exports=jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED \
    -J--add-opens=jdk.compiler/com.sun.tools.javac.code=ALL-UNNAMED \
    -J--add-opens=jdk.compiler/com.sun.tools.javac.comp=ALL-UNNAMED \
    -XDcompilePolicy=simple \
    --should-stop=ifError=FLOW \
    -processorpath /path/to/../error_prone_core-2.36.0-with-dependencies.jar:dataflow-errorprone-3.42.0-eisop5.jar \
    -Xplugin:ErrorProne \
    /path/to/java-file/seed_Prone_102.java
```
You should be able to see the scan results in the console after the command finishes.

### ![](https://cdn.brandfetch.io/idoNI4g3wp/w/16/h/16/theme/dark/icon.jpeg?c=1dxbfHSJFAPEGdCLU4o5B) Running SonarQube

Running SonarQube is not as straightforward as the other two tools. First you need to setup a SonarQube server.
You can checkout [this documentation](https://docs.sonarsource.com/sonarqube-server/latest/try-out-sonarqube/) on how
to setup the SonarQube server.

After the setup is ready and SonarQube server is running, log in to `http://localhost:9000` (or other port if you
change the default) and create a project. For demonstration say the project name you created is `llm-for-sa`.
Next, create a `sonar-project.properties` file with the following contents.
```
sonar.projectKey=llm-for-sa
sonar.sources=/path/to/java-file/
sonar.inclusions=/path/to/java-file/seed_Prone_102.java
sonar.exclusions=**/*.class
sonar.scm.exclusions.disabled=true
sonar.cpd.exclusions=**/*
sonar.sourceEncoding=UTF-8
sonar.scanner.metadata.cleanup=true
sonar.host.url=http://localhost:9000
# update the credentials if changed from default
sonar.login=admin
sonar.password=admin
```
Then change directory to where this file is and run `sonar-scanner` command. After the scan finishes, go back to 
your browser and open http://localhost:9000, and you should be able to see the scan results. -->

